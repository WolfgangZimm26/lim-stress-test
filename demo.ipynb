{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import json\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to establish what could be going wrong with the docker container by breaking the functions in my module apart then testing them seperatly. After that we will test the fast api call and make sure no errors are still occuring. Finally the corret functions and api call will be put into a docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from .env file\n",
    "#use the .env file to not expose our secret API key\n",
    "#load_dotenv()\n",
    "#client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "#    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = os.path.join(os.path.dirname('lim-stress-tes'), '.env')\n",
    "load_dotenv(dotenv_path)\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function is called get_openai_responses. It takes in a prompt and will obtain responses. The number of responses can be controlled by the num_responses. To limit the tokens set the varaible max_token. Initial_delay,exponetial_base are used when a rate limit occurs and it emplements exponetial backing. The function is set to retry 10 times if a rate limit error occurs. Adjust if need be\n",
    "\n",
    "***to test I set the num_responses = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_responses(prompt, num_responses=40, max_tokens=100, initial_delay: float = 1,\n",
    "                         exponential_base: float = 2, jitter: bool = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retrieves a specified number of responses from OpenAI's GPT model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the model.\n",
    "        num_responses (int): The number of responses to retrieve.\n",
    "        max_tokens (int): The maximum number of tokens per response.\n",
    "        initial_delay (float): Initial delay in seconds before retrying after a rate limit error.\n",
    "        exponential_base (float): The base for the exponential backoff calculation.\n",
    "        jitter (bool): Whether to add random jitter to the delay.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of responses from the AI model.\n",
    "    \"\"\"\n",
    "    if num_responses > 100:  # Example threshold\n",
    "        raise ValueError(\"num_responses is too high. Please reduce the number.\")\n",
    "\n",
    "    delay = initial_delay\n",
    "    responses = []\n",
    "    retries = 0\n",
    "    max_retries = 10\n",
    "\n",
    "    while len(responses) < num_responses and retries < max_retries:\n",
    "        try:\n",
    "            AI_response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides recommendations.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                max_tokens=max_tokens,\n",
    "                n=1,  # Number of completions to generate\n",
    "            )\n",
    "            #print(AI_response)\n",
    "            #need to eventually un comment these\n",
    "            response_content = AI_response.choices[0].message.content\n",
    "            responses.append(response_content)\n",
    "        except Exception as e:\n",
    "            print(f\"Encountered an error: {e}\")\n",
    "            if 'rate limit' in str(e).lower():\n",
    "                delay *= exponential_base * (1 + jitter * random.random())\n",
    "                print(f\"Rate limit exceeded. Waiting for {delay} seconds.\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(\"Encountered a non-rate-limit error. Retrying...\")\n",
    "            retries += 1\n",
    "            time.sleep(initial_delay)  # Basic delay for non-rate-limit errors\n",
    "\n",
    "    if retries == max_retries:\n",
    "        print(\"Max retries reached. Some responses may not have been retrieved.\")\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is called get_bert_embedding. As the name states this function will obtain the bertembeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(my_text):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    tokens = tokenizer(my_text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    embedding = torch.mean(last_hidden_states, dim=1)\n",
    "    \n",
    "    # Convert to numpy array and check shape\n",
    "    embedding_np = embedding.numpy()\n",
    "    #print(f\"Embedding shape: {embedding_np.shape}\")  # For debugging\n",
    "    return embedding_np.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function in my module is process_prompt. This function will compare the cosine similarities for each response and compare each response to everyother response created. It will then return the quant scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO Not USE\n",
    "\n",
    "def process_prompt(prompt):\n",
    "    result_data = []  # Store quant scores for all prompts\n",
    "    responses = get_openai_responses(prompt)\n",
    "\n",
    "    prompt_embedding = get_bert_embedding(prompt)\n",
    "    response_embeddings = [get_bert_embedding(response) for response in responses]\n",
    "\n",
    "    cosine_sim_scores = []\n",
    "    embedding_pairs = list(combinations([prompt_embedding] + response_embeddings, 2))\n",
    "    for pair in embedding_pairs:\n",
    "        score = cosine_similarity([pair[0]], [pair[1]])[0][0]\n",
    "        cosine_sim_scores.append(score)\n",
    "\n",
    "    mean_score = np.mean(cosine_sim_scores)\n",
    "    median_score = np.median(cosine_sim_scores)\n",
    "    mode_score = stats.mode(cosine_sim_scores)\n",
    "    average_score = np.average(cosine_sim_scores)\n",
    "    sample_size = len(cosine_sim_scores)\n",
    "\n",
    "    quant_scores = {\n",
    "        \"mean\": mean_score,\n",
    "        \"median\": median_score,\n",
    "        \"mode\": mode_score,\n",
    "        \"average\": average_score,\n",
    "        \"sample_size\": sample_size\n",
    "    }\n",
    "\n",
    "    result_data.append({\"prompt\": prompt, \"quant_scores\": quant_scores})\n",
    "\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original DO NOT USE\n",
    "\n",
    "def process_prompt(prompt):\n",
    "    result_data = []  # Store quant scores for all prompts\n",
    "    responses = get_openai_responses(prompt, num_responses=5)\n",
    "\n",
    "    prompt_embedding = get_bert_embedding(prompt)\n",
    "    response_embeddings = [get_bert_embedding(response) for response in responses]\n",
    "\n",
    "    cosine_sim_scores = []\n",
    "    for response_emb in response_embeddings:\n",
    "        # Compute cosine similarity between prompt and each response\n",
    "        # Since both are 2D arrays, no need to wrap them in another list\n",
    "        score = cosine_similarity(prompt_embedding, response_emb)[0][0]\n",
    "        cosine_sim_scores.append(score)\n",
    "    mean_score = np.mean(cosine_sim_scores)\n",
    "    median_score = np.median(cosine_sim_scores)\n",
    "    mode_result=stats.mode(cosine_sim_scores)\n",
    "    mode_score = mode_result.mode[0]\n",
    "    average_score = np.average(cosine_sim_scores)\n",
    "    sample_size = len(cosine_sim_scores)\n",
    "\n",
    "    quant_scores = {\n",
    "        \"mean\": mean_score,\n",
    "        \"median\": median_score,\n",
    "        \"mode\": mode_score,\n",
    "        \"average\": average_score,\n",
    "        \"sample_size\": sample_size\n",
    "    }\n",
    "\n",
    "    result_data.append({\"prompt\": prompt, \"quant_scores\": quant_scores})\n",
    "    return result_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated and unbugged\n",
    "\n",
    "def process_prompt(prompt):\n",
    "    result_data = []  # Store quant scores for all prompts\n",
    "    responses = get_openai_responses(prompt)\n",
    "\n",
    "    prompt_embedding = get_bert_embedding(prompt)  # This returns a list of lists\n",
    "    response_embeddings = [get_bert_embedding(response) for response in responses]\n",
    "\n",
    "    cosine_sim_scores = []\n",
    "    # Create combinations of the prompt embedding and all response embeddings\n",
    "    response_embeddings = [get_bert_embedding(response) for response in responses]\n",
    "    cosine_sim_scores = []\n",
    "    for emb1, emb2 in combinations(response_embeddings, 2):\n",
    "        # Compute cosine similarity between each pair of response embeddings\n",
    "        score = cosine_similarity(emb1, emb2)[0][0]\n",
    "        cosine_sim_scores.append(score)\n",
    "\n",
    "        # Calculate statistical measures\n",
    "        mean_score = np.mean(cosine_sim_scores)\n",
    "        median_score = np.median(cosine_sim_scores)\n",
    "        mode_result = stats.mode(cosine_sim_scores, keepdims=False)\n",
    "\n",
    "\n",
    "        # Check if the mode result is a scalar or an array and extract the mode value\n",
    "        if np.isscalar(mode_result.mode):\n",
    "            mode_score = mode_result.mode\n",
    "        else:\n",
    "            mode_score = mode_result.mode[0] if mode_result.mode.size else None\n",
    "        average_score = np.average(cosine_sim_scores)\n",
    "    sample_size = len(cosine_sim_scores)\n",
    "\n",
    "    quant_scores = {\n",
    "        \"mean\": mean_score,\n",
    "        \"median\": median_score,\n",
    "        \"mode\": mode_score,\n",
    "        \"average\": average_score,\n",
    "        \"sample_size\": sample_size\n",
    "    }\n",
    "\n",
    "    result_data.append({\"prompt\": prompt, \"quant_scores\": quant_scores})\n",
    "\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test if we can get replies from the three functions in the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Cell For get_openai_responses\n",
    "my_prompt='What is today?'\n",
    "my_results=get_openai_responses(my_prompt, num_responses=5)\n",
    "print(my_prompt)\n",
    "print(my_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for berkembeding function:\n",
    "my_result2=get_bert_embedding(my_results[0])\n",
    "print(my_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is today?\n",
      "[{'prompt': 'What is today?', 'quant_scores': {'mean': 0.8148964012386626, 'median': 0.8164481292113372, 'mode': 0.971654440284831, 'average': 0.8148964012386626, 'sample_size': 780}}]\n"
     ]
    }
   ],
   "source": [
    "#Test Cell\n",
    "my_prompt='What is today?'\n",
    "my_results3=process_prompt(my_prompt)\n",
    "print(my_prompt)\n",
    "print(my_results3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing has been completed and all functions are working as expected. This next cell block will test for version info of our modules.\n",
    "These shall be added to a requirement txt for a dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 4.36.2\n",
      "torch version: 2.1.2+cpu\n",
      "sklearn version: 1.3.2\n",
      "numpy version: 1.24.4\n",
      "scipy version: 1.10.1\n",
      "itertools is a built-in module, so it doesn't have a version.\n",
      "time is a built-in module, so it doesn't have a version.\n",
      "random is a built-in module, so it doesn't have a version.\n",
      "os is a built-in module, so it doesn't have a version.\n",
      "dotenv version: dotenv.main\n",
      "openai version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "# Import the required modules\n",
    "import transformers\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import scipy\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Print the versions\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"sklearn version: {sklearn.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"scipy version: {scipy.__version__}\")\n",
    "print(f\"itertools is a built-in module, so it doesn't have a version.\")\n",
    "print(f\"time is a built-in module, so it doesn't have a version.\")\n",
    "print(f\"random is a built-in module, so it doesn't have a version.\")\n",
    "print(f\"os is a built-in module, so it doesn't have a version.\")\n",
    "print(f\"dotenv version: {load_dotenv.__module__}\")  # For dotenv, we use a different approach\n",
    "\n",
    "# The OpenAI module version might need a different approach since it's often installed via pip\n",
    "# and doesn't always have a __version__ attribute. We use pkg_resources to get the version.\n",
    "try:\n",
    "    import pkg_resources\n",
    "    openai_version = pkg_resources.get_distribution(\"openai\").version\n",
    "    print(f\"openai version: {openai_version}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not determine openai version: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
