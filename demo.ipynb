{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import json\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to establish what could be going wrong with the docker container by breaking the functions in my module apart then testing them seperatly. After that we will test the fast api call and make sure no errors are still occuring. Finally the corret functions and api call will be put into a docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from .env file\n",
    "#use the .env file to not expose our secret API key\n",
    "#load_dotenv()\n",
    "#client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "#    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = os.path.join(os.path.dirname('lim-stress-tes'), '.env')\n",
    "load_dotenv(dotenv_path)\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function is called get_openai_responses. It takes in a prompt and will obtain responses. The number of responses can be controlled by the num_responses. To limit the tokens set the varaible max_token. Initial_delay,exponetial_base are used when a rate limit occurs and it emplements exponetial backing. The function is set to retry 10 times if a rate limit error occurs. Adjust if need be\n",
    "\n",
    "***to test I set the num_responses = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_responses(prompt, num_responses=40, max_tokens=100, initial_delay: float = 1,\n",
    "                         exponential_base: float = 2, jitter: bool = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retrieves a specified number of responses from OpenAI's GPT model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the model.\n",
    "        num_responses (int): The number of responses to retrieve.\n",
    "        max_tokens (int): The maximum number of tokens per response.\n",
    "        initial_delay (float): Initial delay in seconds before retrying after a rate limit error.\n",
    "        exponential_base (float): The base for the exponential backoff calculation.\n",
    "        jitter (bool): Whether to add random jitter to the delay.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of responses from the AI model.\n",
    "    \"\"\"\n",
    "    if num_responses > 100:  # Example threshold\n",
    "        raise ValueError(\"num_responses is too high. Please reduce the number.\")\n",
    "\n",
    "    delay = initial_delay\n",
    "    responses = []\n",
    "    retries = 0\n",
    "    max_retries = 10\n",
    "\n",
    "    while len(responses) < num_responses and retries < max_retries:\n",
    "        try:\n",
    "            AI_response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides recommendations.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                max_tokens=max_tokens,\n",
    "                n=1,  # Number of completions to generate\n",
    "            )\n",
    "            #print(AI_response)\n",
    "            #need to eventually un comment these\n",
    "            response_content = AI_response.choices[0].message.content\n",
    "            responses.append(response_content)\n",
    "        except Exception as e:\n",
    "            print(f\"Encountered an error: {e}\")\n",
    "            if 'rate limit' in str(e).lower():\n",
    "                delay *= exponential_base * (1 + jitter * random.random())\n",
    "                print(f\"Rate limit exceeded. Waiting for {delay} seconds.\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(\"Encountered a non-rate-limit error. Retrying...\")\n",
    "            retries += 1\n",
    "            time.sleep(initial_delay)  # Basic delay for non-rate-limit errors\n",
    "\n",
    "    if retries == max_retries:\n",
    "        print(\"Max retries reached. Some responses may not have been retrieved.\")\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is called get_bert_embedding. As the name states this function will obtain the bertembeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(my_text):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    tokens = tokenizer(my_text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    embedding = torch.mean(last_hidden_states, dim=1)\n",
    "    \n",
    "    # Convert to numpy array and check shape\n",
    "    embedding_np = embedding.numpy()\n",
    "    #print(f\"Embedding shape: {embedding_np.shape}\")  # For debugging\n",
    "    return embedding_np.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function in my module is process_prompt. This function will compare the cosine similarities for each response and compare each response to everyother response created. It will then return the quant scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prompt(prompt):\n",
    "    result_data = []  # Store quant scores for all prompts\n",
    "    responses = get_openai_responses(prompt)\n",
    "\n",
    "    prompt_embedding = get_bert_embedding(prompt)\n",
    "    response_embeddings = [get_bert_embedding(response) for response in responses]\n",
    "\n",
    "    cosine_sim_scores = []\n",
    "    embedding_pairs = list(combinations([prompt_embedding] + response_embeddings, 2))\n",
    "    print(embedding_pairs)\n",
    "    for pair in embedding_pairs:\n",
    "        score = cosine_similarity([pair[0]], [pair[1]])[0][0]\n",
    "        cosine_sim_scores.append(score)\n",
    "\n",
    "    mean_score = np.mean(cosine_sim_scores)\n",
    "    median_score = np.median(cosine_sim_scores)\n",
    "    mode_score = stats.mode(cosine_sim_scores)\n",
    "    average_score = np.average(cosine_sim_scores)\n",
    "    sample_size = len(cosine_sim_scores)\n",
    "\n",
    "    quant_scores = {\n",
    "        \"mean\": mean_score,\n",
    "        \"median\": median_score,\n",
    "        \"mode\": mode_score,\n",
    "        \"average\": average_score,\n",
    "        \"sample_size\": sample_size\n",
    "    }\n",
    "\n",
    "    result_data.append({\"prompt\": prompt, \"quant_scores\": quant_scores})\n",
    "\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test if we can get replies from the three functions in the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Cell For get_openai_responses\n",
    "my_prompt='What is today?'\n",
    "my_results=get_openai_responses(my_prompt, num_responses=5)\n",
    "print(my_prompt)\n",
    "print(my_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for berkembeding function:\n",
    "my_result2=get_bert_embedding(my_results[0])\n",
    "print(my_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Cell\n",
    "my_prompt='What is today?'\n",
    "my_results3=process_prompt(my_prompt)\n",
    "print(my_prompt)\n",
    "print(my_results3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
